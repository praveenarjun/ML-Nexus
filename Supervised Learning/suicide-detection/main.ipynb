{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = ''#paste path of the dataset\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "suicide = df[df['class']=='suicide'].sample(n=5000,random_state=42,replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_suicide = df[df['class']=='non-suicide'].sample(n=5000,random_state=42,replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.concat([suicide,non_suicide],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.reset_index(drop=True,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this Unnamed: 0 column is unnecessary so drop this\n",
    "merged_df = merged_df.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lowercasing\n",
    "merged_df['text'] = merged_df['text'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_urls(text):\n",
    "  url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "  return url_pattern.sub(r'', text)\n",
    "\n",
    "merged_df['text'] = merged_df['text'].apply(remove_urls)\n",
    "merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tags(text):\n",
    "  tag_pattern = re.compile(r'<[^>]+>')\n",
    "  return tag_pattern.sub(r'', text)\n",
    "\n",
    "merged_df['text'] = merged_df['text'].apply(remove_tags)\n",
    "merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "short_hand = {\n",
    "    \"afaik\": \"as far as i know\",\n",
    "    \"afk\": \"away from keyboard\",\n",
    "    \"asap\": \"as soon as possible\",\n",
    "    \"atk\": \"at the keyboard\",\n",
    "    \"atm\": \"at the moment\",\n",
    "    \"a3\": \"anytime, anywhere, anyplace\",\n",
    "    \"bak\": \"back at keyboard\",\n",
    "    \"bbl\": \"be back later\",\n",
    "    \"bbs\": \"be back soon\",\n",
    "    \"bfn\": \"bye for now\",\n",
    "    \"b4n\": \"bye for now\",\n",
    "    \"brb\": \"be right back\",\n",
    "    \"brt\": \"be right there\",\n",
    "    \"btw\": \"by the way\",\n",
    "    \"b4\": \"before\",\n",
    "    \"cu\": \"see you\",\n",
    "    \"cul8r\": \"see you later\",\n",
    "    \"cya\": \"see you\",\n",
    "    \"faq\": \"frequently asked questions\",\n",
    "    \"fc\": \"fingers crossed\",\n",
    "    \"fwiw\": \"for what it's worth\",\n",
    "    \"fyi\": \"for your information\",\n",
    "    \"gal\": \"get a life\",\n",
    "    \"gg\": \"good game\",\n",
    "    \"gn\": \"good night\",\n",
    "    \"gmta\": \"great minds think alike\",\n",
    "    \"gr8\": \"great!\",\n",
    "    \"g9\": \"genius\",\n",
    "    \"ic\": \"i see\",\n",
    "    \"icq\": \"i seek you (also a chat program)\",\n",
    "    \"ilu\": \"i love you\",\n",
    "    \"imho\": \"in my honest/humble opinion\",\n",
    "    \"imo\": \"in my opinion\",\n",
    "    \"iow\": \"in other words\",\n",
    "    \"irl\": \"in real life\",\n",
    "    \"kiss\": \"keep it simple, stupid\",\n",
    "    \"ldr\": \"long distance relationship\",\n",
    "    \"lmao\": \"laugh my a.. off\",\n",
    "    \"lol\": \"laughing out loud\",\n",
    "    \"ltns\": \"long time no see\",\n",
    "    \"l8r\": \"later\",\n",
    "    \"mte\": \"my thoughts exactly\",\n",
    "    \"m8\": \"mate\",\n",
    "    \"nrn\": \"no reply necessary\",\n",
    "    \"oic\": \"oh i see\",\n",
    "    \"pita\": \"pain in the a..\",\n",
    "    \"prt\": \"party\",\n",
    "    \"prw\": \"parents are watching\",\n",
    "    \"qpsa?\": \"que pasa?\",\n",
    "    \"rofl\": \"rolling on the floor laughing\",\n",
    "    \"rofllol\": \"rolling on the floor laughing out loud\",\n",
    "    \"rotflmao\": \"rolling on the floor laughing my a.. off\",\n",
    "    \"sk8\": \"skate\",\n",
    "    \"stats\": \"your sex and age\",\n",
    "    \"asl\": \"age, sex, location\",\n",
    "    \"thx\": \"thank you\",\n",
    "    \"ttfn\": \"ta-ta for now!\",\n",
    "    \"ttyl\": \"talk to you later\",\n",
    "    \"u\": \"you\",\n",
    "    \"u2\": \"you too\",\n",
    "    \"u4e\": \"yours for ever\",\n",
    "    \"wb\": \"welcome back\",\n",
    "    \"wtf\": \"what the f...\",\n",
    "    \"wtg\": \"way to go!\",\n",
    "    \"wuf\": \"where are you from?\",\n",
    "    \"w8\": \"wait...\",\n",
    "    \"7k\": \"sick:-d laugher\",\n",
    "    \"tfw\": \"that feeling when\",\n",
    "    \"mfw\": \"my face when\",\n",
    "    \"mrw\": \"my reaction when\",\n",
    "    \"ifyp\": \"i feel your pain\",\n",
    "    \"tntl\": \"trying not to laugh\",\n",
    "    \"jk\": \"just kidding\",\n",
    "    \"idc\": \"i don’t care\",\n",
    "    \"ily\": \"i love you\",\n",
    "    \"imu\": \"i miss you\",\n",
    "    \"adih\": \"another day in hell\",\n",
    "    \"zzz\": \"sleeping, bored, tired\",\n",
    "    \"wywh\": \"wish you were here\",\n",
    "    \"time\": \"tears in my eyes\",\n",
    "    \"bae\": \"before anyone else\",\n",
    "    \"fimh\": \"forever in my heart\",\n",
    "    \"bsaaw\": \"big smile and a wink\",\n",
    "    \"bwl\": \"bursting with laughter\",\n",
    "    \"bff\": \"best friends forever\",\n",
    "    \"csl\": \"can’t stop laughing\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_shorthand(text):\n",
    "  new_text = []\n",
    "  for word in text.split():\n",
    "    if word in short_hand.keys():\n",
    "      new_text.append(short_hand[word])\n",
    "    else:\n",
    "      new_text.append(word)\n",
    "  return \" \".join(new_text)\n",
    "\n",
    "merged_df['text'] = merged_df['text'].apply(replace_shorthand)\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "contractions = {\n",
    "    \"ain't\": \"am not | is not | are not | has not | have not | did not\",\n",
    "    \"amn't\": \"am not\",\n",
    "    \"aren't\": \"are not | am not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"cain't\": \"cannot\",\n",
    "    \"'cause\": \"because\",\n",
    "    \"cause\": \"because\",\n",
    "    \"could've\": \"could have\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"couldn't've\": \"could not have\",\n",
    "    \"daren't\": \"dare not | dared not\",\n",
    "    \"dasn't\": \"dare not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"doesn't\": \"does not\",\n",
    "    \"don't\": \"do not | does not\",\n",
    "    \"d'ye\": \"do you | did you\",\n",
    "    \"e'er\": \"ever\",\n",
    "    \"everybody's\": \"everybody is\",\n",
    "    \"everyone's\": \"everyone is\",\n",
    "    \"finna\": \"fixing to | going to\",\n",
    "    \"g'day\": \"good day\",\n",
    "    \"gimme\": \"give me\",\n",
    "    \"giv'n\": \"given\",\n",
    "    \"gonna\": \"going to\",\n",
    "    \"gon't\": \"go not\",\n",
    "    \"gotta\": \"got to\",\n",
    "    \"hadn't\": \"had not\",\n",
    "    \"had've\": \"had have\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"he'd\": \"he had | he would\",\n",
    "    \"he'll\": \"he will | he shall\",\n",
    "    \"he's\": \"he has | he is\",\n",
    "    \"he've\": \"he have\",\n",
    "    \"how'd\": \"how did | how would\",\n",
    "    \"howdy\": \"how do you do | how do you fare\",\n",
    "    \"how'll\": \"how will\",\n",
    "    \"how're\": \"how are\",\n",
    "    \"how's\": \"how is | how has | how does\",\n",
    "    \"i'd\": \"i had | i would\",\n",
    "    \"i'd've\": \"i would have\",\n",
    "    \"i'll\": \"i will | i shall\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"i'm'a\": \"i am about to\",\n",
    "    \"imma\": \"i am about to\",\n",
    "    \"i'm'o\": \"i am going to\",\n",
    "    \"innit\": \"is it not\",\n",
    "    \"i've\": \"i have\",\n",
    "    \"i've \":\"i have\",\n",
    "    \"i'm\" : \"i am\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"it'd\": \"it would\",\n",
    "    \"it'll\": \"it will | it shall\",\n",
    "    \"it's\": \"it is | it has\",\n",
    "    \"let's\": \"let us\",\n",
    "    \"ma'am\": \"madam\",\n",
    "    \"mayn't\": \"may not\",\n",
    "    \"may've\": \"may have\",\n",
    "    \"methinks\": \"me thinks\",\n",
    "    \"mightn't\": \"might not\",\n",
    "    \"might've\": \"might have\",\n",
    "    \"mustn't\": \"must not\",\n",
    "    \"mustn't've\": \"must not have\",\n",
    "    \"must've\": \"must have\",\n",
    "    \"needn't\": \"need not\",\n",
    "    \"ne'er\": \"never\",\n",
    "    \"o'clock\": \"of the clock\",\n",
    "    \"o'er\": \"over\",\n",
    "    \"ol'\": \"old\",\n",
    "    \"oughtn't\": \"ought not\",\n",
    "    \"'s\": \"is | has | does | or us\",\n",
    "    \"shalln't\": \"shall not\",\n",
    "    \"shan't\": \"shall not\",\n",
    "    \"she'd\": \"she had | she would\",\n",
    "    \"she'll\": \"she will | she shall\",\n",
    "    \"she's\": \"she is | she has\",\n",
    "    \"should've\": \"should have\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"shouldn't've\": \"should not have\",\n",
    "    \"somebody's\": \"somebody is | somebody has\",\n",
    "    \"someone's\": \"someone is | someone has\",\n",
    "    \"something's\": \"something is | something has\",\n",
    "    \"so're\": \"so are\",\n",
    "    \"that'll\": \"that will | that shall\",\n",
    "    \"that're\": \"that are\",\n",
    "    \"that's\": \"that is | that has\",\n",
    "    \"that'd\": \"that would | that had\",\n",
    "    \"there'd\": \"there had | there would\",\n",
    "    \"there'll\": \"there will | there shall\",\n",
    "    \"there're\": \"there are\",\n",
    "    \"there's\": \"there is | there has\",\n",
    "    \"these're\": \"these are\",\n",
    "    \"these've\": \"these have\",\n",
    "    \"they'd\": \"they had | they would\",\n",
    "    \"they'll\": \"they will | they shall\",\n",
    "    \"they're\": \"they are | they were\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"this's\": \"this is | this has\",\n",
    "    \"those're\": \"those are\",\n",
    "    \"those've\": \"those have\",\n",
    "    \"'tis\": \"it is\",\n",
    "    \"to've\": \"to have\",\n",
    "    \"'twas\": \"it was\",\n",
    "    \"twas\": \"it was\",\n",
    "    \"wanna\": \"want to\",\n",
    "    \"wasn't\": \"was not\",\n",
    "    \"we'd\": \"we had | we would | we did\",\n",
    "    \"we'd've\": \"we would have\",\n",
    "    \"we'll\": \"we will | we shall\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"what'd\": \"what did\",\n",
    "    \"what'll\": \"what will | what shall\",\n",
    "    \"what're\": \"what are | what were\",\n",
    "    \"what's\": \"what is | what has | what does\",\n",
    "    \"what've\": \"what have\",\n",
    "    \"when's\": \"when is\",\n",
    "    \"where'd\": \"where did\",\n",
    "    \"where'll\": \"where will | where shall\",\n",
    "    \"where're\": \"where are\",\n",
    "    \"where's\": \"where is | where has | where does\",\n",
    "    \"where've\": \"where have\",\n",
    "    \"which'd\": \"which had | which would\",\n",
    "    \"which'll\": \"which will | which shall\",\n",
    "    \"which're\": \"which are\",\n",
    "    \"which's\": \"which is | which has\",\n",
    "    \"which've\": \"which have\",\n",
    "    \"who'd\": \"who did | who had | who would\",\n",
    "    \"who'd've\": \"who would have\",\n",
    "    \"who'll\": \"who will | who shall\",\n",
    "    \"who's\": \"who is | who does | who has\",\n",
    "    \"who've\": \"who have\",\n",
    "    \"why'd\": \"why did\",\n",
    "    \"why're\": \"why are\",\n",
    "    \"why's\": \"why is | why does | why has\",\n",
    "    \"won't\": \"will not\",\n",
    "    \"would've\": \"would have\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"y'all\": \"you all\",\n",
    "    \"y'all'd've\": \"you all would have\",\n",
    "    \"y'all're\": \"you all are\",\n",
    "    \"you'd\": \"you would | you had\",\n",
    "    \"you'll\": \"you will | you shall\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"you've\": \"you have\",\n",
    "    \"<noun>'s\": \"<noun> is\"\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_contractions(text):\n",
    "  new_text = []\n",
    "  for word in text.split():\n",
    "    if word in contractions.keys():\n",
    "      new_text.append(contractions[word])\n",
    "    else:\n",
    "      new_text.append(word)\n",
    "  return ' '.join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df['text']=merged_df['text'].apply(remove_contractions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emojis(text):\n",
    "  emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "  return emoji_pattern.sub(r'', text)\n",
    "\n",
    "merged_df['text'] = merged_df['text'].apply(remove_emojis)\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_numbers(text):\n",
    "  return re.sub(r'\\d+', '', text)\n",
    "\n",
    "merged_df['text'] = merged_df['text'].apply(remove_numbers)\n",
    "merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "  return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "merged_df['text'] = merged_df['text'].apply(remove_punctuation)\n",
    "\n",
    "# Remove question marks\n",
    "merged_df['text'] = merged_df['text'].str.replace('?', '')\n",
    "\n",
    "merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_stopwords(text):\n",
    "  return \" \".join([word for word in str(text).split() if word not in stop_words])\n",
    "\n",
    "\n",
    "merged_df['text'] = merged_df['text'].apply(remove_stopwords)\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')\n",
    "\n",
    "def tokenize_text(text):\n",
    "  return word_tokenize(text)\n",
    "\n",
    "merged_df['text'] = merged_df['text'].apply(tokenize_text)\n",
    "merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "  return [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "merged_df['text'] = merged_df['text'].apply(stem_tokens)\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_tokens(tokens):\n",
    "  return [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "merged_df['text'] = merged_df['text'].apply(lemmatize_tokens)\n",
    "merged_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "merged_df['text'] = merged_df['text'].apply(lambda x: ' '.join(x))\n",
    "\n",
    "# Creating a TF-IDF vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fitting and transforming the text data\n",
    "tfidf_matrix = vectorizer.fit_transform(merged_df['text'])\n",
    "\n",
    "# Printing the shape of the TF-IDF matrix\n",
    "print(tfidf_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_matrix, merged_df['class'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Checking the shape of the split data\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Training the Logistic Regression model\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Model evaluation\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Training SVM model\n",
    "svm_model = SVC(kernel='linear')\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions\n",
    "y_pred_svm = svm_model.predict(X_test)\n",
    "\n",
    "# Model Evaluation\n",
    "print(\"SVM Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
    "print(\"SVM Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_svm))\n",
    "print(\"SVM Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "\n",
    "# Training SVM model\n",
    "svm_model = SVC(kernel='rbf')\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions\n",
    "y_pred_svm = svm_model.predict(X_test)\n",
    "\n",
    "print(\"SVM Accuracy:\", accuracy_score(y_test, y_pred_svm))\n",
    "print(\"SVM Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_svm))\n",
    "print(\"SVM Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Training Decision Tree model\n",
    "dt_model = DecisionTreeClassifier(random_state=42)\n",
    "dt_model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions\n",
    "y_pred_dt = dt_model.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "print(\"Decision Tree Accuracy:\", accuracy_score(y_test, y_pred_dt))\n",
    "print(\"Decision Tree Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_dt))\n",
    "print(\"Decision Tree Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# Training AdaBoost model\n",
    "adaboost_model = AdaBoostClassifier(n_estimators=100, random_state=42)\n",
    "adaboost_model.fit(X_train, y_train)\n",
    "\n",
    "# Making predictions\n",
    "y_pred_ada = adaboost_model.predict(X_test)\n",
    "\n",
    "# Evaluating the model\n",
    "print(\"AdaBoost Accuracy:\", accuracy_score(y_test, y_pred_ada))\n",
    "print(\"AdaBoost Classification Report:\")\n",
    "print(classification_report(y_test, y_pred_ada))\n",
    "print(\"AdaBoost Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred_ada))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
